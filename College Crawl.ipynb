{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import time\n",
    "class CollegeCrawl(object):\n",
    "    allUrls={}\n",
    "    visitedUrls=set()\n",
    "    rejectedUrls=set()\n",
    "    n_allUrls=0\n",
    "    n_sitedUrls=0\n",
    "    n_rejectedUrls=0\n",
    "    \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: www.university.edu\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then every page \n",
    "        respectrobottxt: True\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, _prioritykeywords,gap_Insecond=5,max_pages=150, _respectrobottxt=True):\n",
    "        self.college=_collegename\n",
    "        self.rootUrl=_rooturl\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "        self.gap_Insecond=gap_Insecond\n",
    "        self.max_Pages=max_pages\n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def all_pages(self, links_only=True):\n",
    "        headers={'User-Agent':'Mozilla/5.0'}\n",
    "        response=requests.get(self.rootUrl,headers=headers)\n",
    "        allUrls={self.rootUrl}\n",
    "        visitedUrls=set()\n",
    "        rejectedUrls=set()\n",
    "        while len(allUrls)>len(visitedUrls)+len(rejectedUrls) and len(visitedUrls) < self.max_Pages:\n",
    "            soup=BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                try:\n",
    "                    url=link['href']                \n",
    "                    parsed_uri = urlparse(url )\n",
    "                    if parsed_uri.netloc=='':\n",
    "                        absolute_url=self.rootUrl+url    \n",
    "                    elif parsed_uri.netloc==base_domain:\n",
    "                        absolute_url=url\n",
    "                    else:\n",
    "                        continue\n",
    "                    clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url).replace('.edu','.edu/').replace('.edu//','.edu/').replace('.edu//','.edu/')\n",
    "                    allUrls.add(clean)               \n",
    "                except:\n",
    "                    continue\n",
    "            unvisited_url=(allUrls-visitedUrls-rejectedUrls).pop()\n",
    "            response=requests.get(unvisited_url)\n",
    "            if response.status_code!=200:\n",
    "                rejectedUrls.add(unvisited_url)  \n",
    "            else:\n",
    "                visitedUrls.add(unvisited_url) #only creates csv files for visited urls\n",
    "            time.sleep(self.gap_Insecond)\n",
    "        return [allUrls,visitedUrls,rejectedUrls]\n",
    "    \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def Read_OneUrl(self, oneUrl):\n",
    "        response=requests.get(oneUrl)  \n",
    "        if response.status_code==200: \n",
    "            return self.get_pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "    def tag_visible(self,element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "    def get_pagetext(self,body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "                 t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "                 t.nextSibling.name if t.nextSibling!=None else None,\n",
    "                 re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    def Save_OnePage(self, url,folder=None,filename=None,format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.Read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "        if folder==None:\n",
    "            folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder does not exist')\n",
    "            return\n",
    "        if filename==None:\n",
    "            filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_').replace('?','_q_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='',encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "        \n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
    "        return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def Save_Summaries(self,urls):\n",
    "        if(len(list(urls[1]))==150):\n",
    "            print('There were at least ',len(list(urls[0])),' total urls in ',self.rootUrl)\n",
    "        else:\n",
    "            print('There were ',len(list(urls[0])),' total urls in ',self.rootUrl)\n",
    "        print('There were ',len(list(urls[1])),' visited urls in ',self.rootUrl)\n",
    "        print('There were ',len(list(urls[2])),' rejected urls in ',self.rootUrl)\n",
    "        if(len(list(urls[2]))>0):\n",
    "            print('These were the rejected urls: ',list(urls[2]))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  34  total urls in  https://mit.edu/\n",
      "There were  27  visited urls in  https://mit.edu/\n",
      "There were  7  rejected urls in  https://mit.edu/\n",
      "These were the rejected urls:  ['https://mit.edu/mailto:admissions@mit.edu/', 'https://mit.edu/ http://global.mit.edu/', 'https://mit.edu/tel:617-253-1000', 'https://mit.edu/mailto:dataprotection@mit.edu/', 'https://mit.edu/mailto:aacomments@mit.edu/', 'https://mit.edu/mailto:dataprotection@mit.edu/.', 'https://mit.edu/mailto:campus-map@mit.edu/']\n",
      "There were  55  total urls in  https://yale.edu/\n",
      "There were  52  visited urls in  https://yale.edu/\n",
      "There were  3  rejected urls in  https://yale.edu/\n",
      "These were the rejected urls:  ['https://yale.edu/mailto:titleix@yale.edu/', 'https://yale.edu/mailto:ocr.boston@ed.gov', 'https://yale.edu/mailto:employee.services@yale.edu/']\n",
      "There were  2  total urls in  https://washington.edu/\n",
      "There were  2  visited urls in  https://washington.edu/\n",
      "There were  0  rejected urls in  https://washington.edu/\n",
      "There were  21  total urls in  https://stanford.edu/\n",
      "There were  1  visited urls in  https://stanford.edu/\n",
      "There were  20  rejected urls in  https://stanford.edu/\n",
      "These were the rejected urls:  ['https://stanford.edu/contact', 'https://stanford.edu/health-care/', 'https://stanford.edu/about', 'https://stanford.edu/list/continuing/', 'https://stanford.edu/#main-content', 'https://stanford.edu/#content', 'https://stanford.edu/site/privacy', 'https://stanford.edu/atoz/', 'https://stanford.edu/list/research/', 'https://stanford.edu/(none)', 'https://stanford.edu/site/terms/', 'https://stanford.edu/campus-life/', 'https://stanford.edu/list/academic/', 'https://stanford.edu/academics', 'https://stanford.edu/list/interdisc/', 'https://stanford.edu/site/accessibility/', 'https://stanford.edu/research', 'https://stanford.edu/', 'https://stanford.edu/about/history', 'https://stanford.edu/admission']\n",
      "There were at least  255  total urls in  https://harvard.edu/\n",
      "There were  150  visited urls in  https://harvard.edu/\n",
      "There were  12  rejected urls in  https://harvard.edu/\n",
      "These were the rejected urls:  ['https://harvard.edu/tel:617-495-1585', 'https://harvard.edu/mailto:digitalcomms@harvard.edu/', 'https://harvard.edu/mailto:info_center@harvard.edu/', 'https://harvard.edu/mailto:digitalcomms@harvard.edu/?Subject=Harvard%20Website%20Feedback', 'https://harvard.edu/mailto:media@harvard.edu/', 'https://harvard.edu/mailto:president@harvard.edu/', 'https://harvard.edu/mailto:?subject=Harvard%20President%20News:%20Results%20of%20Harvard%20AAU%20Student%20Survey%20on%20Sexual%20Assault%20and%20Misconduct&body=October%2015%2C%202019%20%0D%0A%0D%0ARead%20More:%20http%3A%2F%2Fwww.harvard.edu/%2Fpresident%2Fnews%2F2019%2Fresults-harvard-aau-student-survey-on-sexual-assault-and-misconduct', 'https://harvard.edu/mailto:trademark_program@harvard.edu/', 'https://harvard.edu/mailto:photo_services@harvard.edu/', 'https://harvard.edu/mailto:?subject=Harvard%20President%20News:%20Contact&body=July%201%2C%202011%20%0D%0A%0D%0ARead%20More:%20http%3A%2F%2Fwww.harvard.edu/%2Fpresident%2Fcontact', 'https://harvard.edu/mailto:visitor_center@harvard.edu/', 'https://harvard.edu/tel:617-495-1573']\n"
     ]
    }
   ],
   "source": [
    "names=['MIT','Yale','University of Washington','Stanford','Harvard']\n",
    "urls=['https://mit.edu/','https://yale.edu/','https://washington.edu/','https://stanford.edu/','https://harvard.edu/']\n",
    "keywords=['apply,admission']\n",
    "wait_time=0 #can change this to 5 when we need. I set it to 0 to speed things up. Default value is 5 if you don't input it\n",
    "max_pages=150#default value is 150 if you don't input it\n",
    "for college in range(len(names)):\n",
    "    c=CollegeCrawl(names[college],urls[college],keywords,wait_time,max_pages)\n",
    "    pages=c.all_pages()\n",
    "    for page in list(pages[1]):\n",
    "        c.Save_OnePage(page)\n",
    "    c.Save_Summaries(pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
