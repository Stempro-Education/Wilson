{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint\n",
    "import selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "def all_pages(base_url, links_only=True):\n",
    "    response=requests.get(base_url)\n",
    "    unique_urls={base_url}\n",
    "    visited_urls=set()\n",
    "    unables=set()\n",
    "    while len(unique_urls)>len(visited_urls):\n",
    "        soup=BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            try:\n",
    "                url=link['href']                \n",
    "                parsed_uri = urlparse(url )\n",
    "                if parsed_uri.netloc=='':\n",
    "                    absolute_url=base_url+url    \n",
    "                elif parsed_uri.netloc==base_domain:\n",
    "                    absolute_url=url\n",
    "                else:\n",
    "                    continue\n",
    "                clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url).replace('.edu','.edu/').replace('.edu//','.edu/')\n",
    "                unique_urls.add(clean)               \n",
    "            except:\n",
    "                continue\n",
    "        unvisited_url=(unique_urls-visited_urls-unables).pop()\n",
    "        visited_urls.add(unvisited_url)\n",
    "        response=requests.get(unvisited_url)\n",
    "        if links_only!=True and response.status_code=='200':\n",
    "            handleOnePage(unvisited_url)\n",
    "        else:\n",
    "            unables.add(unvisited_url)  \n",
    "    return unique_urls\n",
    "def read_oneurl(url):  \n",
    "    response=requests.get(url)  \n",
    "    if response.status_code==200: \n",
    "        return get_pagetext(response)\n",
    "    else: \n",
    "        return [[None, None, None, None]]\n",
    "    \n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True \n",
    "\n",
    "def get_pagetext(body):\n",
    "    soup = BeautifulSoup(body.text, 'html.parser')\n",
    "    texts = soup.findAll(text=True) \n",
    "    visible_texts = filter(tag_visible, texts)       \n",
    "    \n",
    "    return [ [t.parent.name,   \n",
    "             t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "             t.nextSibling.name if t.nextSibling!=None else None,\n",
    "             re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "\n",
    "def save_tocsv(url,folder=None, filename=None): # tab delimiter only\n",
    "    url=url.strip()\n",
    "    content=read_oneurl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "    if folder==None:\n",
    "        folder=os.getcwd()\n",
    "    if path.isdir(folder)==False:\n",
    "        print('folder doesnot exist')\n",
    "        return\n",
    "    if filename==None:\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "    fullname=path.join(folder, filename)\n",
    "    with open(fullname, 'w', newline='',encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "        \n",
    "        for lll in content: \n",
    "            lll.insert(0, url)\n",
    "            writer.writerow(lll)  \n",
    "    return fullname           \n",
    "for i in range(4):\n",
    "    a=all_pages('http://'+input('input website: '))\n",
    "    for x in a:\n",
    "        save_tocsv(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
