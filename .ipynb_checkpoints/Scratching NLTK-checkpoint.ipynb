{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=darkgreen> (400 Points)Scratching NLTK: 400/400</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:00:23.520067Z",
     "start_time": "2020-02-09T05:00:18.597204Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:01:14.881283Z",
     "start_time": "2020-02-09T05:01:14.876284Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:01:16.676839Z",
     "start_time": "2020-02-09T05:01:16.540334Z"
    }
   },
   "outputs": [],
   "source": [
    "def GetWordlist(url='https://www.nme.com/features/greta-thunberg-full-speech-to-the-un-2019-climate-change-summit-2550824'):\n",
    "    words=[]\n",
    "    response=requests.get(url, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    soup=BeautifulSoup(response.text,  'lxml') #'html.parser')  \n",
    "    for item in soup.find_all('p'):  \n",
    "        words.append(item)\n",
    "    return words\n",
    "stuff=GetWordlist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:04:30.998696Z",
     "start_time": "2020-02-09T05:04:30.882708Z"
    }
   },
   "source": [
    "<font size=6 color=orange>The content is extracted. </font>\n",
    "<b><br>You may need to add request, re and bs4.Beautifulsoup librarys. \n",
    "<br>line 6:  words.append(item) --> how about  words.append(item.text?)\n",
    "print('\\n'.join(words))\n",
    "</b>    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:10:09.640220Z",
     "start_time": "2020-02-09T05:10:09.635227Z"
    }
   },
   "outputs": [],
   "source": [
    "words=[]\n",
    "for i in stuff:\n",
    "    s=str(i)\n",
    "    if s[0:3]=='<p>':\n",
    "        text=s[3:-4]\n",
    "        if len(text) > 0 and text[0]!='<':\n",
    "            if text[0] == '\"' or text[0] == '“':\n",
    "                text=text[1:-1]\n",
    "            while text.find('<') != -1:\n",
    "                start=text.find('<')\n",
    "                stop=text.find('>')\n",
    "                text=text[:start]+text[stop+1:]\n",
    "            words.append(text)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:10:32.371927Z",
     "start_time": "2020-02-09T05:10:32.367912Z"
    }
   },
   "source": [
    "<font size=6 color=green>fluent use of list</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:12:18.085478Z",
     "start_time": "2020-02-09T05:12:18.082476Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_words=[]\n",
    "for i in words:\n",
    "    i=re.sub(r'[.,()!?]','',i).split()\n",
    "    for word in i:\n",
    "        word=word.lower()\n",
    "        list_of_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:13:25.872112Z",
     "start_time": "2020-02-09T05:13:25.867075Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:12:47.018258Z",
     "start_time": "2020-02-09T05:12:46.969304Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Alphabetical sort:')\n",
    "print()\n",
    "list_of_words.sort()\n",
    "for w in list_of_words:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:40:47.019780Z",
     "start_time": "2020-02-09T05:40:46.988780Z"
    }
   },
   "outputs": [],
   "source": [
    "unique={}\n",
    "print('By frequency:')\n",
    "for word in list_of_words:\n",
    "    l=list_of_words.count(word)\n",
    "    unique.update([[word,l]])\n",
    "t=list(set(sorted(unique.values(),reverse=True)))\n",
    "t.reverse()\n",
    "for i in t:\n",
    "    for word in unique:\n",
    "        if(unique[word]) == i:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:23:07.076786Z",
     "start_time": "2020-02-09T05:23:07.070792Z"
    }
   },
   "source": [
    "<font size=6 color=green>excellent. good understanding of dictionary and list</font> \n",
    "<br> <b> how about use Counter from collections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:23:22.723686Z",
     "start_time": "2020-02-09T05:23:22.719685Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:24:10.294326Z",
     "start_time": "2020-02-09T05:24:10.261331Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma=list(unique)\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "print('Word\\t\\t\\t\\tStem')\n",
    "print()\n",
    "for i in lemma:\n",
    "    f=int(len(i)/8)\n",
    "    if f==2:\n",
    "        print(i+'\\t\\t'+porter.stem(i))\n",
    "    elif f==1:\n",
    "        print(i+'\\t\\t\\t'+porter.stem(i))\n",
    "    else:\n",
    "        print(i+'\\t\\t\\t\\t'+porter.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:26:50.481301Z",
     "start_time": "2020-02-09T05:26:50.477273Z"
    }
   },
   "source": [
    "<font size=6 color=green>excellent. wow. good format .... good try\n",
    "<br> may also consider the following: .</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:39:51.494490Z",
     "start_time": "2020-02-09T05:39:51.460533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Word\tStem            \n",
      "               10\t10               \n",
      "             15°c\t15°c             \n",
      "      16-year-old\t16-year-old      \n",
      "              1st\t1st              \n",
      "             2018\t2018             \n",
      "               30\t30               \n",
      "              350\t350              \n",
      "              420\t420              \n",
      "              50%\t50%              \n",
      "              67%\t67%              \n",
      "                a\ta                \n",
      "            about\tabout            \n",
      "       acceptable\taccept           \n",
      "              act\tact              \n",
      "           action\taction           \n",
      "         activist\tactivist         \n",
      "       additional\taddit            \n",
      "              air\tair              \n",
      "              all\tall              \n",
      "          already\talreadi          \n",
      "             also\talso             \n",
      "               am\tam               \n",
      "              and\tand              \n",
      "            angry\tangri            \n",
      "              any\tani              \n",
      "              are\tare              \n",
      "               as\tas               \n",
      "          aspects\taspect           \n",
      "               at\tat               \n",
      "             away\taway             \n",
      "             back\tback             \n",
      "           barely\tbare             \n",
      "               be\tbe               \n",
      "          because\tbecaus           \n",
      "             been\tbeen             \n",
      "        beginning\tbegin            \n",
      "          believe\tbeliev           \n",
      "            below\tbelow            \n",
      "             best\tbest             \n",
      "         betrayal\tbetray           \n",
      "           beyond\tbeyond           \n",
      "         billions\tbillion          \n",
      "           budget\tbudget           \n",
      "         business\tbusi             \n",
      "              but\tbut              \n",
      "               by\tby               \n",
      "              can\tcan              \n",
      "            chain\tchain            \n",
      "           chance\tchanc            \n",
      "           change\tchang            \n",
      "        childhood\tchildhood        \n",
      "           choose\tchoos            \n",
      "            clear\tclear            \n",
      "          climate\tclimat           \n",
      "              co2\tco2              \n",
      "       collapsing\tcollaps          \n",
      "             come\tcome             \n",
      "           coming\tcome             \n",
      "     consequences\tconsequ          \n",
      "         continue\tcontinu          \n",
      "          control\tcontrol          \n",
      "          crystal\tcrystal          \n",
      "          cutting\tcut              \n",
      "             dare\tdare             \n",
      "               do\tdo               \n",
      "            doing\tdo               \n",
      "            don’t\tdon’t            \n",
      "             down\tdown             \n",
      "             draw\tdraw             \n",
      "           dreams\tdream            \n",
      "            dying\tdie              \n",
      "         economic\teconom           \n",
      "       ecosystems\tecosystem        \n",
      "eight-and-a0-half\teight-and-a0-half\n",
      "         emission\temiss            \n",
      "        emissions\temiss            \n",
      "             emit\temit             \n",
      "            empty\tempti            \n",
      "           enough\tenough           \n",
      "           entire\tentir            \n",
      "         entirely\tentir            \n",
      "           equity\tequiti           \n",
      "          eternal\tetern            \n",
      "         everyone\teveryon          \n",
      "             evil\tevil             \n",
      "            exist\texist            \n",
      "       extinction\textinct          \n",
      "             eyes\teye              \n",
      "             fail\tfail             \n",
      "          failing\tfail             \n",
      "            fairy\tfairi            \n",
      "         feedback\tfeedback         \n",
      "           figure\tfigur            \n",
      "          figures\tfigur            \n",
      "              for\tfor              \n",
      "          forgive\tforgiv           \n",
      "             full\tfull             \n",
      "           future\tfutur            \n",
      "       generation\tgener            \n",
      "      generations\tgener            \n",
      "              get\tget              \n",
      "         gigatons\tgigaton          \n",
      "            given\tgiven            \n",
      "            gives\tgive             \n",
      "           global\tglobal           \n",
      "             gone\tgone             \n",
      "            greta\tgreta            \n",
      "           growth\tgrowth           \n",
      "              had\thad              \n",
      "             half\thalf             \n",
      "              has\tha               \n",
      "             have\thave             \n",
      "             hear\thear             \n",
      "        heartfelt\theartfelt        \n",
      "              her\ther              \n",
      "             here\there             \n",
      "           hidden\thidden           \n",
      "             hope\thope             \n",
      "              how\thow              \n",
      "            human\thuman            \n",
      "         hundreds\thundr            \n",
      "                i\ti                \n",
      "             idea\tidea             \n",
      "               if\tif               \n",
      "      impassioned\timpass           \n",
      "               in\tin               \n",
      "          include\tinclud           \n",
      "       incredible\tincred           \n",
      "             ipcc\tipcc             \n",
      "     irreversible\tirrevers         \n",
      "               is\tis               \n",
      "               it\tit               \n",
      "              i’m\ti’m              \n",
      "          january\tjanuari          \n",
      "          justice\tjustic           \n",
      "             kept\tkept             \n",
      "          leaders\tleader           \n",
      "             less\tless             \n",
      "              let\tlet              \n",
      "           levels\tlevel            \n",
      "             like\tlike             \n",
      "             line\tline             \n",
      "             live\tlive             \n",
      "             look\tlook             \n",
      "            loops\tloop             \n",
      "            lucky\tlucki            \n",
      "             made\tmade             \n",
      "             mass\tmass             \n",
      "           matter\tmatter           \n",
      "           mature\tmatur            \n",
      "              may\tmay              \n",
      "          message\tmessag           \n",
      "            money\tmoney            \n",
      "             more\tmore             \n",
      "             most\tmost             \n",
      "               my\tmy               \n",
      "           needed\tneed             \n",
      "            never\tnever            \n",
      "               no\tno               \n",
      "              not\tnot              \n",
      "              now\tnow              \n",
      "          nowhere\tnowher           \n",
      "          numbers\tnumber           \n",
      "            ocean\tocean            \n",
      "             odds\todd              \n",
      "               of\tof               \n",
      "              off\toff              \n",
      "               on\ton               \n",
      "              one\tone              \n",
      "             ones\tone              \n",
      "             only\tonli             \n",
      "               or\tor               \n",
      "            other\tother            \n",
      "              our\tour              \n",
      "              out\tout              \n",
      "           people\tpeopl            \n",
      "            plans\tplan             \n",
      "             plea\tplea             \n",
      "           points\tpoint            \n",
      "         politics\tpolit            \n",
      "        pollution\tpollut           \n",
      "          popular\tpopular          \n",
      "        presented\tpresent          \n",
      "          pretend\tpretend          \n",
      "        reactions\treaction         \n",
      "           really\trealli           \n",
      "           refuse\trefus            \n",
      "             rely\treli             \n",
      "        remaining\tremain           \n",
      "            right\tright            \n",
      "             rise\trise             \n",
      "             risk\trisk             \n",
      "            risks\trisk             \n",
      "              sad\tsad              \n",
      "              say\tsay              \n",
      "           school\tschool           \n",
      "          science\tscienc           \n",
      "          setting\tset              \n",
      "              she\tshe              \n",
      "           should\tshould           \n",
      "        shouldn’t\tshouldn’t        \n",
      "             side\tside             \n",
      "            sight\tsight            \n",
      "           simply\tsimpli           \n",
      "        situation\tsituat           \n",
      "               so\tso               \n",
      "        solutions\tsolut            \n",
      "           solved\tsolv             \n",
      "             some\tsome             \n",
      "           speech\tspeech           \n",
      "         starting\tstart            \n",
      "          staying\tstay             \n",
      "            still\tstill            \n",
      "           stolen\tstolen           \n",
      "          sucking\tsuck             \n",
      "        suffering\tsuffer           \n",
      "           summit\tsummit           \n",
      "          swedish\tswedish          \n",
      "             take\ttake             \n",
      "            tales\ttale             \n",
      "             talk\ttalk             \n",
      "          talking\ttalk             \n",
      "        technical\ttechnic          \n",
      "     technologies\ttechnolog        \n",
      "             tell\ttell             \n",
      "      temperature\ttemperatur       \n",
      "             than\tthan             \n",
      "            thank\tthank            \n",
      "             that\tthat             \n",
      "              the\tthe              \n",
      "             then\tthen             \n",
      "            there\tthere            \n",
      "            these\tthese            \n",
      "             they\tthey             \n",
      "             this\tthi              \n",
      "            those\tthose            \n",
      "       thunberg’s\tthunberg’        \n",
      "          tipping\ttip              \n",
      "               to\tto               \n",
      "            today\ttoday            \n",
      "          today’s\ttoday’           \n",
      "             tons\tton              \n",
      "              too\ttoo              \n",
      "          towards\ttoward           \n",
      "            toxic\ttoxic            \n",
      "               un\tun               \n",
      "    uncomfortable\tuncomfort        \n",
      "       understand\tunderstand       \n",
      "       understood\tunderstood       \n",
      "               up\tup               \n",
      "             upon\tupon             \n",
      "          urgency\turgenc           \n",
      "               us\tus               \n",
      "            usual\tusual            \n",
      "           waking\twake             \n",
      "             want\twant             \n",
      "          warming\twarm             \n",
      "         watching\twatch            \n",
      "               we\twe               \n",
      "            we’ll\twe’ll            \n",
      "             when\twhen             \n",
      "            where\twhere            \n",
      "          whether\twhether          \n",
      "              who\twho              \n",
      "             will\twill             \n",
      "             with\twith             \n",
      "            words\tword             \n",
      "            world\tworld            \n",
      "            would\twould            \n",
      "            wrong\twrong            \n",
      "            years\tyear             \n",
      "        yesterday\tyesterday        \n",
      "              yet\tyet              \n",
      "              you\tyou              \n",
      "            young\tyoung            \n",
      "             your\tyour             \n"
     ]
    }
   ],
   "source": [
    "lemma=list(unique)\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "# we need the longest word in the dictionary. otherwise, the format will be messed up\n",
    "longest=len(max(unique, key=len))  \n",
    "print('Word'.rjust(longest)+'\\tStem'.ljust(longest))\n",
    "for i in lemma:\n",
    "    print(i.rjust(longest)+'\\t'+porter.stem(i).ljust(longest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:37:02.049632Z",
     "start_time": "2020-02-09T05:37:02.022636Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"30 difficult words\\nNote: Some words are not difficult, there weren't enough difficult words\\n\")\n",
    "hwords=['urgency','toxic','technologies','sucking','situation','setting','presented','plea','nowhere','odds','matter','leaders','justice','kept','irreversible','impassioned','heartfelt','gigatons','extinction','equity','eternal','crystal','consequences','childhood','betrayal','billions','aspects','additional','activist']\n",
    "from nltk.corpus import wordnet as wn\n",
    "for w in hwords: \n",
    "    syns = wordnet.synsets(w)\n",
    "    if len(syns) > 0:\n",
    "        sy=''\n",
    "        ant=''\n",
    "        for i in syns:\n",
    "            for l in i.lemmas(): \n",
    "                if l.antonyms(): \n",
    "                    ant=(l.antonyms()[0].name()) \n",
    "            sy=str(i.name())[:-5]\n",
    "            if w == sy:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if ant=='':\n",
    "            print(w, '\\nsynonym: ', sy,'\\nantonym:',ant,'None\\n')\n",
    "        else:\n",
    "            print(w, '\\nsynonym: ', sy,'\\nantonym:',ant,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:37:09.331386Z",
     "start_time": "2020-02-09T05:37:09.325395Z"
    }
   },
   "source": [
    "<font size=6 color=green>Excellent! </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
