{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import selenium\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import sys\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "import csv\n",
    "class CollegeCrawl(object):\n",
    "    gap_Insecond=5\n",
    "    max_Pages=150\n",
    "    allUrls={}\n",
    "    visitedUrls=set()\n",
    "    rejectedUrls=set()\n",
    "    n_allUrls=0\n",
    "    n_sitedUrls=0\n",
    "    n_rejectedUrls=0\n",
    "    \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: www.university.edu\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then every page \n",
    "        respectrobottxt: True\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, _prioritykeywords, _respectrobottxt=True):\n",
    "        self.college=_collegename\n",
    "        self.rootUrl=_rooturl\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "\n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl\n",
    "    \"\"\"\n",
    "    def all_pages(self, base_url, links_only=True):\n",
    "        response=requests.get(base_url)\n",
    "        allUrls={base_url}\n",
    "        visitedUrls=set()\n",
    "        rejectedUrls=set()\n",
    "        while len(allUrls)>len(visitedUrls):\n",
    "            soup=BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                try:\n",
    "                    url=link['href']                \n",
    "                    parsed_uri = urlparse(url )\n",
    "                    if parsed_uri.netloc=='':\n",
    "                        absolute_url=base_url+url    \n",
    "                    elif parsed_uri.netloc==base_domain:\n",
    "                        absolute_url=url\n",
    "                    else:\n",
    "                        continue\n",
    "                    clean=re.sub(r'[_+!@#$?\\\\\\s]+$', '', absolute_url).replace('.edu','.edu/').replace('.edu//','.edu/')\n",
    "                    allUrls.add(clean)               \n",
    "                except:\n",
    "                    continue\n",
    "            unvisited_url=(allUrls-visitedUrls-rejectedUrls).pop()\n",
    "            visitedUrls.add(unvisited_url)\n",
    "            response=requests.get(unvisited_url)\n",
    "            if response.status_code!=200:\n",
    "                rejectedUrls.add(unvisited_url)  \n",
    "        return [allUrls[0:max_Pages],visitedUrls,rejectedUrls] if len(allUrls)>150 else [allUrls,visitedUrls,rejectedUrls]\n",
    "    \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def Read_OneUrl(self, oneUrl):\n",
    "        response=requests.get(oneUrl)  \n",
    "        if response.status_code==200: \n",
    "            return self.get_pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "    def tag_visible(self,element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "    def get_pagetext(self,body):\n",
    "        soup = BeautifulSoup(body.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.tag_visible, texts)       \n",
    "    \n",
    "        return [ [t.parent.name,   \n",
    "                 t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "                 t.nextSibling.name if t.nextSibling!=None else None,\n",
    "                 re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2] \n",
    "    \"\"\"\n",
    "        Save One Page\n",
    "    \"\"\"\n",
    "    def Save_OnePage(self, url,folder=None,filename=None,format='csv'):\n",
    "        url=url.strip()\n",
    "        content=self.Read_OneUrl(url) #in format of (a,a,a,a)\n",
    "     \n",
    "        if folder==None:\n",
    "            folder=os.getcwd()\n",
    "        if path.isdir(folder)==False:\n",
    "            print('folder does not exist')\n",
    "            return\n",
    "        if filename==None:\n",
    "            filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "    \n",
    "        fullname=path.join(folder, filename)\n",
    "        with open(fullname, 'w', newline='',encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "        \n",
    "            for lll in content: \n",
    "                lll.insert(0, url)\n",
    "                writer.writerow(lll)  \n",
    "        return fullname           \n",
    "    \n",
    "    \"\"\"\n",
    "        Save summaries\n",
    "    \"\"\"\n",
    "    def Save_Summaries(self,urls):\n",
    "        print('There were ',len(list(urls[1]))+len(list(urls[2])),' total urls in ',self.rootUrl)\n",
    "        print('There were ',len(list(urls[1])),' visited urls in ',self.rootUrl)\n",
    "        print('There were ',len(list(urls[2])),' rejected urls in ',self.rootUrl)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were  42  total urls in  http://mit.edu/\n",
      "There were  35  visited urls in  http://mit.edu/\n",
      "There were  7  rejected urls in  http://mit.edu/\n",
      "There were  76  total urls in  http://yale.edu/\n",
      "There were  73  visited urls in  http://yale.edu/\n",
      "There were  3  rejected urls in  http://yale.edu/\n",
      "There were  2  total urls in  http://washington.edu/\n",
      "There were  2  visited urls in  http://washington.edu/\n",
      "There were  0  rejected urls in  http://washington.edu/\n",
      "There were  2  total urls in  http://stanford.edu/\n",
      "There were  1  visited urls in  http://stanford.edu/\n",
      "There were  1  rejected urls in  http://stanford.edu/\n"
     ]
    }
   ],
   "source": [
    "names=['MIT','Yale','University of Washington','Stanford']\n",
    "urls=['http://mit.edu/','http://yale.edu/','http://washington.edu/','http://stanford.edu/']\n",
    "k=['apply,admission']\n",
    "for q in range(4):\n",
    "    c=CollegeCrawl(names[q],urls[q],k)\n",
    "    a=c.all_pages(urls[q])\n",
    "    for x in list(a[1]):\n",
    "        c.Save_OnePage(x)\n",
    "    c.Save_Summaries(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
